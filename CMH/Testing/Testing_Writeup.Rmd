---
title: "Early warning signals derived from COVID-19 testing data"
author: "Christopher M. Hoover, Joshua Schwab, Maya Peterson, Mark van der Laan, others"
csl: "../../../plos.csl"
bibliography: "c19ref.bib"
output: 
  pdf_document: 
    keep_tex: true
  word_document:
    reference_docx: "../../../choover_word_styles.docx"  
geometry: margin=2cm 
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE,
                      message = FALSE,
                      warning = FALSE)

require(adaptivetau)
require(parallel)
require(deSolve)
require(dagitty)
require(EpiEstim)
require(surveillance)
require(tidyverse)

```

## Abstract  
COVID19 testing data is essential to monitor the status of ongoing epidemics across the U.S. and the world. Because of the high rate of asymptomatic and mildly symptomatic cases that contribute to transmission, widespread testing and subsequent contact tracing and isolation are necessary to reduce epidemic spread. The effective reproductive rate, $\mathcal{R}_e$, is a simple measure of transmission interpreted as the expected number of new infections arising from a single infection at a given time, that can be used to monitor the status of an outbreak. Here we discuss estimation of $\mathcal{R}_e$ from currently available testing data and proprose methods to correct for inherent biases that currently limit the utility of testing as a reliable indicator of the status of the COVID19 pandemic.  

# TODO:  
* Update DAG based on Maya and Mark email chain, incorporate time ordering  
  - How does $\mathbb{E}(Y(t))/\mathbb{E}(Y(t-7))$ (or something similar) relate to $\mathcal{R}_e$?  
* Decide on data generating mechanism (ABM?)  
* **With line list data** adjust testing for demographics, zip code, SES, age  
* Estimate $\mathcal{R}_e$ from testing data adjusted for underreporting using Hospitalizations/Deaths data ([LSHTM method](https://cmmid.github.io/topics/covid19/global_cfr_estimates.html))  

# Background   
## COVID19  
Areas across the United States and the world are beginning to reopen following the unprecedented pandemic caused by the emergence of SARS-CoV2. Widespread availability of testing for active and past infections remains a critical component of the ongoing response to the COVID19 pandemic...  
 
## Testing   
* Background on types of tests  
* What tests are actually doing (e.g. distingiush PCR = active infections, antibody = past infections)  
  - Sensitivity issues  

### Testing can be earliest indicator of rising infections if conducted/analyzed appropriately  
Hospitalizations and deaths are more what we're worried about, but these are preceded in time by increase in cases. Since tests can detect infection before these more severe outcomes, they can be an earlier indicator of increases in transmission. A useful summary statistic to estimate the current status of an outbreak is $\mathcal{R}_e$, the expected number of additional cases caused by a single case at any time during the outbreak [@gosticPracticalConsiderationsMeasuring2020]. 

### Testing cannot currently be used to reliably estimate $\mathcal{R}_e$ because of bias due to variability in test-seeking behaviors, test availability (geographically, socioeconomically, etc.)  
Testing can be used in either passive or active surveillance to monitor the status of ongoing outbreaks. Especially early in the outbreak when testing was not widely available, the set of individuals allowed (wc?) to be tested was restricted. To date, most COVID19 testing data has been passively collected: those with suspected COVID19 exposure or symptoms seek out testing from a healthcare provider, rather than actively being pursued to be tested in a more systematic fashion, e.g. as part of a representative cohort of the population of interest. This introduces considerable bias into epidemiological estimates derived from the testing data, such as $\mathcal{R}_e$. [Review CITEs on passive surveillance and bias] The number of tests available, test-seeking behaviors of different populations/individuals, and [something else] all influence the number of positive tests and the positive test percent in a given location and time (Figure 1, W2; **Maybe could add $\mathcal{R}_e$ to the DAG as a function of $Y$?**). 

Crude metrics from testing data such as the number of positive tests reported or the percent of tests conducted that are positive have been/are being used to monitor the status of outbreaks across the US/world. However, such metrics are heavily biased due to reasons outlines above, and may be flawed representations of true underlying infection dynamics such as $\mathcal{R}_e$ [**Think this sentence/point is really important and could use some work**]. Here we propose/review methods for correcting for these underlying biases in testing data, discuss their pros and cons, and evaluate their ability to a) estimate $\mathcal{R}_e$ in real time and b) predict a future increase in hospitalizations that will overwhelm capacity. We conclude by proposing methods to generate unbiased testing data prospectively using adaptive design (active surveillance, embedding prospective cohorts within passive testing, all/none of the above?) and [hopefully] show that these methods outperform those currently available that rely on inherent flaws in currently available testing data. 

```{r dag, fig.cap = "TODO: Make this longitudinal? Data generating process for COVID19 testing data where W1 is demographic covariates (age, race, SES), W2 is risk covariates (symptoms, contacts), Y is SARS-CoV2 infection status, delta is testing status. The observed data $O$ consists of ($W_1, delta, dW2, dY$), but conditioing on $W_2$ is necessary to identify $E(Y)$. "}
dag <- dagitty("dag {
W1 -> W2
W1 -> Y
W2 -> Y
W2 -> dW2
W2 -> delta
Y -> dY
dW2 -> dY
delta -> dW2
delta -> dY
}")

dagitty::coordinates( dag ) <-
  list( x=c(W1=0, W2=0, delta=1, Y = 2, dW2=2, dY=3),
        y=c(W1=-3, W2=-2, delta=-1, Y = -2.5, dW2=-2, dY=-2) )
plot( dag )
```

# Correcting for bias/Accurate estimation of $E(Y)$/Adjusting for $W2$   
```{r tab1}
method1 <- list("Method" = "Number of positive tests",
                "Pros" = "real-time, simple, no analysis required",
                "Cons" = "Heavily biased by number of tests conducted, test-seeking behaviors, etc",
                "Reference" = "")

method2 <- list("Method" = "Percent of tests conducted that are positive",
                "Pros" = "Real-time, simple, removes bias from number of tests conducted",
                "Cons" = "Heavily biased by test-seeking behaviors, etc.",
                "Reference" = "")

method3 <- list("Method" = "Adjustment from additional epidemiological data",
                "Pros" = "Removes bias associated with test-seeking",
                "Cons" = "Delay between infection and hospitalization/death means unable to estimate in real-time",
                "Reference" = "LSHTM")

method4 <- list("Method" = "Adjustment from additional demographic/survey? data",
                "Pros" = "Simple and straightforward",
                "Cons" = "Does not adequately control for confounding by W2, requires linelist testing data",
                "Reference" = "")

method5 <- list("Method" = "Fitting dynamic models",
                "Pros" = "Real-time, draws on multiple data sources to control for biases",
                "Cons" = "Identifiable?, complex and computationally intensive",
                "Reference" = "")


tab1_df <- data.frame(matrix(unlist(list(method1, method2, method3, method4, method5)), 
                             ncol=4, byrow=T))

colnames(tab1_df) <- names(method1)

tab1_df %>% 
  knitr::kable(format = "latex", booktabs = TRUE,
               caption = "Methods to estimate E(Y)") %>% 
  kableExtra::kable_styling(latex_options = "scale_down")
```

## Approaches  
### Data-generating process  
Needs to:  

* Accurately represent influence of W1 and W2 on transmission  
* Accurately represent influence of W1 and W2 on testing (both simulated and observed)  
* Allow easy manipulation/implementation of changes in $\mathcal{R}_e$  
* Track infection and testing status across different (SES, race, demographic, occupational?) groups  

#### Candidates:  
* Agent-based model  
* Branching-process model with network influence  
* Something simpler may be adequate for the point that testing isn't great and could be improved  

### Estimation from raw testing data  
Either number of positive tests or pct positive tests

### Adjustment from additional data sources (demographic [Age, Race, Zip, SES] and/or epidemiologic [hospitalizations and deaths])  

#### LSHTM method (Case-fatality ratio adjustment?)  
* Time-delay limits utility  
* See [this LSHTM report](https://cmmid.github.io/topics/covid19/global_cfr_estimates.html) for one method that seems to be widely accepted  
* I worked briefly on trying to derive a max likelihood estimate of COVID19 cases on day t from hospitalizations and deaths, but got stuck on right censoring. Could revive that effort though  

### Adjustment built into model fitting  
Big question: How inherently different is this really from methods above? Just a combination of them? 

#### Approach Chris has been working on   
* Still relies on additional data sources for fitting    
* Projections require assumptions of number of tests, bias in testing in the future  




# Improving testing (Is this another paper even??)   

* What additional data can be reported to improve epi estimates (Symptom status, age, etc.)   
* Can additional data be collected in order to improve the utility of available testing resources  
  - Intuition is that smart reallocation of testing to do some active surveillance will improve estimation even more than just doing more tests, which I think could/should be one of the main messages  
  - Could maybe start with data from the Mission Study, use it to derive age/race/income? adjusted rates for all of SF, compare to testing data reported for all of SF county at the same time. Could provide an initial estimate of testing bias. Would be even better if we could do record linkage between Mission Study and other testing data  
  
  
  
# Potential Figures  
```{r get_data}
#Downloads statewide data into dataframes called CA_cases, CA_tests, CA_hosp
source("../Data/Get_COVID_Cal_latest.R")

SF.pop <- 883305

```

## SF Testing  
```{r sf_test_plot}
sf_test_plot_df <- sf_test %>% 
  mutate(tests_10k = (tests/SF.pop)*10000,
         tests_7dayavg = zoo::rollmean(tests,7,na.pad = T,align = "right"),
         tests10k_7dayavg = zoo::rollmean(tests_10k,7,na.pad = T,align = "right"),
         pct_7day_avg = zoo::rollmean(pct*100,7,na.pad = T,align = "right")) %>%
  dplyr::select(Date, tests10k_7dayavg, pct_7day_avg) %>% 
  pivot_longer(-Date, names_to = "Test", values_to = "Num")

test_plot <- sf_test_plot_df %>% 
  ggplot() +
    geom_line(aes(x = Date, y = Num, col = Test),
              size = 1.1) +
    theme_bw() +
    scale_color_manual(values = c("purple", "gold"),
                       labels = c("% Positive",
                                  "Tests\nconducted/10k")) +
    scale_x_date(date_labels = "%m/%d", 
               date_breaks = "7 day") +
    theme(axis.title = element_text(size = 14,
                                    face = "bold"),
          axis.text = element_text(size = 12),
          axis.text.x = element_text(angle = 45,
                                     hjust = 1)) +
    labs(x = "Date",
         y = "Tests and %Positive (7-day average)",
         col = "",
         title = "COVID19 testing in SF")

test_plot

```

## $\mathcal{R}_e$ estimation  
Cori et al method [@coriNewFrameworkSoftware2013; @gosticPracticalConsiderationsMeasuring2020]

```{r cori_Re}
# Get times for estimation
sf_inc <- as.numeric(sf_all %>% filter(Date >as.Date("2020-03-05")) %>% pull(pos) %>% na.omit())
window = 2
ts <- 2:(length(sf_inc)-(window+1))
te <- 2:(length(sf_inc)-(window+1))+window

R_config <- EpiEstim::make_config(t_start = ts,
                                  t_end = te,
                                  mean_si = 7,
                                  std_si = 4)

SF_cori_R <- EpiEstim::estimate_R(incid = sf_inc,
                                  method = "parametric_si",
                                  config = R_config)

SF_cori_R$R %>%
  mutate(Date = as.Date("2020-03-05")+t_start) %>% 
  ggplot() +
    theme_bw() +
    geom_col(data = sf_case,
             aes(x = Date, y = (Cases/SF.pop)*1e5),
             fill = "blue", col = "darkblue", alpha = 0.3) +
    geom_line(aes(x = Date, y = `Median(R)`),
              size = 1.2, col = "red") +
    geom_ribbon(aes(x = Date, y = `Median(R)`,
                   ymin = `Quantile.0.025(R)`,
                   ymax = `Quantile.0.975(R)`),
                fill = "red", alpha = 0.4) +
    geom_hline(yintercept = 1, lty = 2) +
    scale_x_date(date_labels = "%m/%d", 
               date_breaks = "7 day") +
    theme(axis.title = element_text(size = 14,
                                    face = "bold"),
          axis.text = element_text(size = 12),
          axis.text.x = element_text(angle = 45,
                                     hjust = 1)) +
    scale_y_continuous(breaks = c(1,3,5,7)) +
    labs(x = "Date",
         y = expression(paste(R[e], " estimate/cases reported per 100k SF residents")),
         title = "Cori et al estimate of R through time from SF testing data")

```  
  
## Underreporting estimates via LSHTM method  

```{r LSHTM_underreporting}
# Reproducing analysis presented at https://cmmid.github.io/topics/covid19/global_cfr_estimates.html for Bay area counties 

# ---------------------------------------------------------
# Functions from repository
# setting functions for the delay distribution
muTransform <- function(zMedian){
  mu <- log(zMedian) 
}

sigmaTransform <- function(zMean, mu){
  sigma <- sqrt(2*(log(zMean) - mu))
}

# Hospitalisation to death distribution
hospitalisation_to_death_truncated <- function(x, mu, sigma) {
  plnorm(x + 1, mu, sigma) - plnorm(x, mu, sigma)
}

hospitalisation_to_death_truncated_low <- function(x){
  hospitalisation_to_death_truncated(x, muLow, sigmaLow)
}

hospitalisation_to_death_truncated_mid <- function(x){
  hospitalisation_to_death_truncated(x, muMid, sigmaMid)
}

hospitalisation_to_death_truncated_high <- function(x){
  hospitalisation_to_death_truncated(x, muHigh, sigmaHigh)
}

# Function to work out correction CFR
scale_cfr <- function(data_1_in, delay_fun){
  case_incidence <- data_1_in$new_cases
  death_incidence <- data_1_in$new_deaths
  cumulative_known_t <- 0 # cumulative cases with known outcome at time tt
  # Sum over cases up to time tt
  for(ii in 1:nrow(data_1_in)){
    known_i <- 0 # number of cases with known outcome at time ii
    for(jj in 0:(ii - 1)){
      known_jj <- (case_incidence[ii - jj]*delay_fun(jj))
      known_i <- known_i + known_jj
    }
    cumulative_known_t <- cumulative_known_t + known_i # Tally cumulative known
  }
  # naive CFR value
  b_tt <- sum(death_incidence)/sum(case_incidence) 
  # corrected CFR estimator
  p_tt <- sum(death_incidence)/cumulative_known_t
  data.frame(nCFR = b_tt, cCFR = p_tt, total_deaths = sum(death_incidence), 
             cum_known_t = round(cumulative_known_t), total_cases = sum(case_incidence))
}

# working out under-reporting estimate and CIs
underReportingEstimates <- function(data, delay_fun){ 
  dplyr::group_by(data, county) %>%
    dplyr::do(scale_cfr(., delay_fun)) %>%
    dplyr::filter(cum_known_t > 0 & cum_known_t >= total_deaths)  %>%
    dplyr::mutate(nCFR_UQ = binom.test(total_deaths, total_cases)$conf.int[2],
                  nCFR_LQ = binom.test(total_deaths, total_cases)$conf.int[1],
                  cCFR_UQ = binom.test(total_deaths, cum_known_t)$conf.int[2],
                  cCFR_LQ = binom.test(total_deaths, cum_known_t)$conf.int[1],
                  underreporting_estimate = cCFRBaseline / (100*cCFR),
                  lower = cCFREstimateRange[1] / (100 * cCFR_UQ),
                  upper = cCFREstimateRange[2] / (100 * cCFR_LQ),
                  quantile25 = binom.test(total_deaths, cum_known_t, conf.level = 0.5)$conf.int[1],
                  quantile75 = binom.test(total_deaths, cum_known_t, conf.level = 0.5)$conf.int[2]) %>% 
    dplyr::filter(total_deaths > 10)}

# ---------------------------------------------------------

# ---------------------------------------------------------
# Code to run estimates from repository
# ---------------------------------------------------------

# setting the baseline CFR
cCFRBaseline <- 1.4
cCFREstimateRange <- c(1.2, 1.7)

#TODO: Update these e.g. from Joe's paper?
# set parameters of delay distribution -----------------------------------

# lower end of the range
zmeanLow <- 8.7
zmedianLow <- 6.7
muLow <- muTransform(zmedianLow)
sigmaLow <- sigmaTransform(zmeanLow, muLow)

# middle of the range
zmeanMid <- 13
zmedianMid <- 9.1
muMid <- muTransform(zmedianMid)
sigmaMid <- sigmaTransform(zmeanMid, muMid)

# upper end of the range
zmeanHigh <- 20.9
zmedianHigh <- 13.7
muHigh <- muTransform(zmedianHigh)
sigmaHigh <- sigmaTransform(zmeanHigh, muHigh)

# data clean for function
allTogetherClean <- CA_cases %>% 
  mutate(date = as.Date(date, format = "%Y-%m-%d")) %>% 
  dplyr::rename(new_cases = newcountconfirmed,
                new_deaths = newcountdeaths) %>%
  filter(county != "Unassigned" & 
           county != "Out Of Country",
           totalcountdeaths > 10 & 
           new_deaths >= 0 & 
           new_cases >= 0) %>% 
  dplyr::select(county, date, new_cases, new_deaths) %>% 
  padr::pad() %>% 
  drop_na()

# calculate table of estimates using three delay distributions (both ends of the reported ranges and the mean)
allTogetherLow <- underReportingEstimates(allTogetherClean, hospitalisation_to_death_truncated_low) 
allTogetherMid <- underReportingEstimates(allTogetherClean, hospitalisation_to_death_truncated_mid) 
allTogetherHigh <- underReportingEstimates(allTogetherClean, hospitalisation_to_death_truncated_high)

# choosing CIs such that they include all uncertainty from delay distribution
finalRes <- dplyr::tibble(
  county = allTogetherMid$county,
  total_cases = allTogetherMid$total_cases,
  total_deaths = allTogetherMid$total_deaths,
  underreporting_estimate  = pmin(allTogetherLow$underreporting_estimate, allTogetherMid$underreporting_estimate, allTogetherHigh$underreporting_estimate),
  lower = pmin(allTogetherLow$lower, allTogetherMid$lower, allTogetherHigh$lower),
  upper = pmax(allTogetherLow$upper, allTogetherMid$upper, allTogetherHigh$upper))

# Put estimation into a function to estimate through time  
underReportingDate <- function(e_date, data){
  useDat <- data %>% filter(date <= as.Date(e_date))
  # calculate table of estimates using three delay distributions (both ends of the reported ranges and the mean)
    allTogetherLow <- underReportingEstimates(useDat, hospitalisation_to_death_truncated_low) 
    allTogetherMid <- underReportingEstimates(useDat, hospitalisation_to_death_truncated_mid) 
    allTogetherHigh <- underReportingEstimates(useDat, hospitalisation_to_death_truncated_high)

  # choosing CIs such that they include all uncertainty from delay distribution
    outDat <- dplyr::tibble(
      county = allTogetherMid$county,
      date = as.Date(e_date),
      total_cases = allTogetherMid$total_cases,
      total_deaths = allTogetherMid$total_deaths,
      underreporting_estimate  = pmin(allTogetherLow$underreporting_estimate, allTogetherMid$underreporting_estimate, allTogetherHigh$underreporting_estimate),
      lower = pmin(allTogetherLow$lower, allTogetherMid$lower, allTogetherHigh$lower),
      upper = pmax(allTogetherLow$upper, allTogetherMid$upper, allTogetherHigh$upper))
    
    return(outDat)
}

CAunderReporting <- bind_rows(lapply(seq.Date(from = as.Date("2020-04-01"), 
                                              to = Sys.Date()-14, 
                                              by = 1),
                                     underReportingDate,
                                     data = allTogetherClean))

CAunderReporting %>% 
  filter(county %in% c("Alameda", "San Francisco", 
                       "Contra Costa", "Santa Clara",
                       "Sacramento", "Los Angeles", 
                       "Riverside", "Orange", "San Diego")) %>% 
ggplot() +
  theme_bw() +
  geom_ribbon(aes(x = date, ymin = lower*100, ymax = upper*100),
              alpha = 0.2, fill = "blue") +  
  geom_line(aes(x = date, y = underreporting_estimate*100),
            size = 1.2, col = "darkblue") +
  facet_wrap(.~county, ncol = 3, nrow = 3) +
  scale_x_date(date_labels = "%m/%d", 
               date_breaks = "14 day") +
  scale_y_continuous(breaks = c(0,25,50,75,100),
                     limits = c(0,125)) +
    theme(axis.title = element_text(size = 14,
                                    face = "bold"),
          axis.text = element_text(size = 12),
          axis.text.x = element_text(angle = 45,
                                     hjust = 1)) +
    labs(x = "Date",
         y = "% Cases reported",
         title = "Reporting estimates in CA counties",
         subtitle = "  Estimates based on adjustment from deaths by LSHTM method")
```

These estimates come from data from the statewide database, but some wonkiness in there (e.g. negative case counts over the weekends sometime), so will reestimate for SF using the county data.

```{r sf_case_adjust_underreporting}
SF_LSHTM_data <- sf_case %>% 
  dplyr::mutate(county = "San Francisco") %>% 
  dplyr::rename(new_cases = Cases,
                new_deaths = Deaths,
                date = Date) %>% 
  dplyr::select(county, date, new_cases, new_deaths)


SFunderReporting <- bind_rows(lapply(seq.Date(from = as.Date("2020-04-01"), 
                                              to = Sys.Date()-14, 
                                              by = 1),
                                     underReportingDate,
                                     data = SF_LSHTM_data))

SF_all2 <- sf_all %>% 
  dplyr::left_join(as.data.frame(SFunderReporting), by = c("Date" = "date"))

SF_all2 %>% 
  dplyr::select(Date, tests, pos, pct, Cases, underreporting_estimate) %>% 
  pivot_longer(-Date, names_to = "Metric",
               values_to = "Value") %>% 
  ggplot(aes(x = Date, y = Value)) +
    theme_bw() +
    geom_line(size = 1.2) +
    facet_wrap(Metric~., scales = "free_y", ncol = 1)

SF_all2 %>% 
  mutate(adj_cases = total_cases*(1+(1-underreporting_estimate))) %>% 
  pivot_longer(c(total_cases, adj_cases),
               names_to = "Adjust") %>% 
  mutate(Cases_p100k = value/SF.pop*1e5) %>% 
  ggplot() +
    geom_line(aes(x = Date, y = Cases_p100k, col = Adjust)) +
    theme_bw()
  

```

## Back projection from hospitalizations  
#### TODO: Generate case estimates based on proportion of cases that beome hospitalized  
#### TODO: Methods for age, race, ethnicity adjustment if given line list data  

```{r backproj, fig.width=8, fig.height=6, fig.align="left"}
#Function to plot estimates from bootstrapped back projections
backproj.plot <- function(dat.vec, sts.obj){
  sts.sum <- as_tibble(t(apply(sts.obj@lambda, 1, function(x){
    t.sum <- summary(x[1,])
    return(c(t.sum[c(2,3,5)]))
  }))) %>% 
    mutate(obs = dat.vec,
           time = dplyr::row_number())
  
  sts.plot <- sts.sum %>% 
  ggplot() +
    theme_bw() +
    geom_col(aes(x = time, y = obs),
             fill = "blue", col = "darkblue", alpha = 0.8) +
    geom_line(aes(x = time, y = Median), size = 1.2) +
    geom_ribbon(aes(x = time, ymin = `1st Qu.`, ymax = `3rd Qu.`),
                fill = "grey60", alpha = 0.3) +
    labs(x = "Time",
         y = "Hospital admissions and estimated infection series",
         title = "Backprojection estimates of infection times from Hospital Admissions")
  
  return(sts.plot)
}

# Load distribution of times from onset to hospitalization
load("../Data/t_i_h_gamma.Rdata")

#PMF of the time from infection to hospitalization truncated at thirty days
dmax <- 30
inc.pmf <- c(0,dgamma(c(1:dmax), t_i_h_gamma_pars[1], t_i_h_gamma_pars[2]))

#Function to sample from the incubation time
rincu <- function(n) {
  sample(0:dmax, size=n, replace=TRUE, prob=inc.pmf)
}

# Hospital admissions (the "onsets") as an sts object
CA_H_Adm <- CA_hosp %>% 
  mutate(Date = as.Date(todays_date, format = "%Y-%m-%d"),
         H_admits = previous_days_covid_confirmed_patients,
         H_suspects = previous_days_suspected_covid_patients) %>% 
  dplyr::select(Date, county, H_admits, H_suspects) %>% 
  filter(!is.na(H_admits) & !is.na(Date)) %>% 
  arrange(county, Date)

SF_H <- CA_H_Adm %>% 
  filter(county == "San Francisco") %>% 
  pull(H_admits)

SF_H_pad0 <- c(rep(0,dmax), SF_H)

#Call non-parametric back-projection function 
bpnp.control <- list(k=2,
                     eps=rep(0.0001,2),
                     iter.max=rep(500,2),
                     B=500,
                     eq3a.method="C")

SF_H_sts <- new("sts", 
                epoch=1:length(SF_H_pad0),
                observed=matrix(SF_H_pad0,ncol=1))
  
SF_H_bp <- backprojNP(SF_H_sts, incu.pmf=inc.pmf,
                      control=bpnp.control)

SF_H_bp_plot <- backproj.plot(SF_H_pad0, SF_H_bp)

SF_H_bp_plot
```


# References  
