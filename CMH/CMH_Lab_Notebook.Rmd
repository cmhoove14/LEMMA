---
title: "Chris Lab Notebook"
author: "Chris Hoover"
date: "5/11/2020"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = TRUE)

require(rstan)
require(tidyverse)
devtools::load_all("../")
```

# 5/11/2020  
Goals:  
* Familiarize with model  
* Code model in STAN and try some initial fits to get idea of runtimes, outputs     
* Start brainstorming potential improvements to model  

## Package/Model functionality  

### `R/InputsFromSpreadsheets.R`  
Contains functions to read user inputs and translate them into model inputs, generate parameter sets to sample from. Also main wrapper function that users run to generate report: `CredibilityIntervalFromExcel`

### `R/CredibilityInterval.R`  
contains functions to run model, check which model runs "fit" input data (e.g. which parameter sets generate predictions that agree with input data)  

### `R/CombinedModels.R`  
contains functions to run seir model

### `R/CreateOutputs.R`  
Takes model posteriors and generate plots of projections, posterior distributions and comparison to prior distributions  

### Overall process  
1. User calls `CredibilityIntervalFromExcel` which processes data in excel spreadsheet and then calls `CredibilityInterval`  
2. `CredibilityInterval` does some more preprocessing of model inputs and then calls `RunSim1`  
3. `RunSim1` calls `RunSim` which uses `FitSEIR` to generate reasonable estimates of when the outbreak started based on comparison of model generated outputs to observed data done with `CalcError` function. Once FitSEIR has found a best start data for each parameter set, `Seir` is run.  
4. The output of `RunSim1` comes from running `Seir` which produces model estiamtes from the input parameter estimate and best guess start date. `InBounds` is then called to determine if the output of `Seir` is within the user provided uncertainty bounds of the observed data.  
** There seems to be some sort of iteration updating going on that I can't quite figure out  

## Test Run  
```{r test_mod_run, eval = FALSE}
file.copy(system.file("extdata", "SF-April13.xlsx", package = "LEMMA", mustWork = TRUE), "example.xlsx")

LEMMA::CredibilityIntervalFromExcel("example.xlsx")
```

Works for the most part except for a fairly obscure ggplot error  

# 5/15/2020  
Didn't make it to STAN on the 11th, so want to try and get an idea of some of the advantages and disadvantages STAN has compared to other approaches and maybe get around to coding up a version of the model in STAN

## STAN Pros:  
* Hamiltonian MCMC in STAN is quite fast and would generate more posterior samples  
* Could incorporate priors to constrain some of the parameters like hospitalization rate  
* Could place a prior on $t_0$ and fit epidemic start time more explicitly  

## STAN Cons:  
* Not sure how to implement the same type of model fit in STAN that incorporates the min and max guess (i.e. confirmed hospitalizations and hospitalizations + 30% suspected cases). Could put a distribution on it, but could end up being a bit wonky  

# 5/18/2020  
Fixed some algebra slip-ups in the $\mathcal{R}_0$ calculation for the model in the `LEMMA_model` document  

# 5/19/2020  
Working on some initial network-based agent based models. read Maya and Mark's `Adaptive Surveillance` doc. From a coding perspective, making the model run quickly and having fairly flexible implementation of different interventions (e.g. testing, isolating) will be the most challenging. From a practical standpoint, having a realistic contact/network structure may be the most challenging. The `EpiModel` package seems like a good place to start, gonna toy around with that a bit now. Specifically, going to try constructing a stochastic network model with the package which uses the `tergm` and `networkDynamic` packages to implement separable-temporal exponential-family random graph models (STERGMs; whatever that means...) Basically following the [tutorial on the EpiModel website](http://statnet.org/tut/BasicNet.html)   

## Working with `EpiModel` package   
* Good background on networks and terminology [here](http://statnet.org/nme/d1-s6.pdf)  
  * Network characteristics to consider:  
    * **degree distribution**: distribution of number of connections per node   
    * **geodesic distribution**: distribution of the shortest path between nodes  
    * **component size distribution**: distribution of the size of different components if multiple unconnected components in the network  
    
### Step 1) Develop network characteristics  
#### Set network size and characteristics  
```{r EpiModel_first, message=FALSE}
require(EpiModel)

# Initialize network with n=1000
N <- 1000

nw <- network.initialize(n = N, directed = FALSE)

# Add characteristics to the vertexes of the network
  # add race characteristic; here 500 A and 500 W 
  nw <- set.vertex.attribute(nw, "race", c(rep("A", 500), rep("W", 500)))
  # Add household characteristic (assumes mean HH size of 5, this could be more data-informed)
  nw <- set.vertex.attribute(nw, "house", sample(c(1:200), 1000, replace = T))
```

NOTE: could add these characteristics from a separate data frame in which household, race, occupation, dormitory, major, etc. could be correlated and drawn from an empirical distribution of some sort

#### Set network partnership formation formula  
Here we declare characteristics of the network in order to generate edges. This consists of a `formation` formula that declares relationships between nodes based on their characteristics and a `target.stats` vector which gives the expected number of edges given the number of nodes, the network characteristics between nodes (e.g. their mean degree) and their relationships as declared in the `formation` formula.

```{r network_partnership}
formation <- ~edges + nodefactor("race") + nodematch("race") + concurrent

tot.mean.degree <- 2
raceA.mean.degree <- 1
prob.same.race.edge <- 0.75
prob.concurrent <- 0.7

target.stats <- c(N/2*tot.mean.degree, 500*raceA.mean.degree, (N/2*tot.mean.degree)*prob.same.race.edge, prob.concurrent*N)
```

#### Get network dissolution formula  
Rate at which network dissolves 

```{r dissolution_formula}
coef.diss <- dissolution_coefs(dissolution = ~offset(edges), duration = 10)
coef.diss
```

#### Fit network  
```{r fit_net, eval = FALSE}
netfit1 <- netest(nw, 
                  formation = formation, 
                  target.stats = target.stats,
                  coef.diss = coef.diss,
                  edapprox = FALSE)
```

The above took forever to run, and I'm also beginning to question whether all this fuss is really worth it when the contact networks we want to simulate aren't necessarily all that complicated. `EpiModel` seems to be more suited for e.g. sexually transmitted diseases where contact networks relevant to transmission are clearly defined and are fairly dynamic and structured through time. For SARS-CoV2, relationships that lead to contacts that lead to transmission probably fall into one of two categories: highly persistent (e.g. households) or highly random (e.g. the person that I passed on the street that wasn't wearing a mask)

# 5/20/20  
Bummed that I spent the entire day trying to get the `EpiModel` package to work only to give up on it, but I think it's best to just start with an ABM from scratch at this point. There are a couple good places to start, including in Maya and Mark's document, but also [this code from epirecipe](http://epirecip.es/epicookbook/chapters/karlsson/intro) and [this covid abm from oxford](https://github.com/BDI-pathogens/OpenABM-Covid19/blob/master/documentation/covid19_model.pdf)

Started coding the ABM (`COVID_ABM_V1.R`). Have code to generate a network as a matrix, but would be good to inform network characteristics with more data. Would be even better to code so that users could input network characteristics and resulting transmission probabilities and then simulate from there. Could be implemented as a spreadsheet with three columns: number of edges in this part of the network, transmission probability along these edges, and now I've forgotten the third one... Maybe specific transmission characteristics associated with each part of the network? Anyways, basically finished code to generate network structure, now working on simulating transmission on top of the network and then will incorporate interventions.

# 5/21/20  
Added transmission parameters, created to do list for ABM below as well as some initial description of the ABM (see below). Also started working on implementing transmission across the network. Next step is to implement dynamic updating of the network based on interventions, testing, and infection status

# 5/22/20  
Working on updating network based on interventions. Then will work on implementing testing and its effects. After that, can begin adding nuance like some workplaces remaining open, individual heterogeneities, etc. Basic model runs with transmission occurring across network, interventions affecting network. Now need to add effects of infection on network, e.g. severely infectious people stop interacting with most everyone and then add testing and contact tracing effects on network. 

# ABM TODOs  
## Network TODOs  
* Inform random network generation with individual characteristics, e.g. some individuals more "social": higher probability of generating random edges  
* Introduce heterogeneity in workplace network: e.g. some work networks dissolved under shelter in place, others persist (essential workers). Maybe have one type of workplace be healthcare workers whcih specifically interact with severely symptomatic individuals?    
* Implement infection influence on network  
* Look at network characteristics like mean degree, degree distribution  
* Implement testing and isolation on network  

## Transmission TODOs  
* Transmission to households influenced by size/income of household (e.g. ability to isolate within home)  
* Check $\mathcal{R}_0$ in relation to network and transmission probabilities. Pretty sure largest eigen value of the network matrix with entries equal to transmission probabilities can be interpreted as $\mathcal{R}_0$ in which case we have $\mathcal{R}_0\approx5.4$, which is maybe a bit high but in the right ballpark.  
* Introduce isolation and testing  

## Other TODOs  
* Convert some processes into functions for parsimony  
* Introduce nighttime/daytime connections depending on time step, dt. E.g. if dt = 1/2 would have 1 active period, one inactive period per day, if dt = 1/3 two active periods, one inactive per day, and so on  
* Ways to speed up? Transfer some/all functions to `C++`, do some profiling, etc.  

# ABM Description  
## Network structure  
Individuals can be members of different networks such as work, family, school that are constant through time in the absence of interventions. They also participate in random networks that are generated at every time step based on parameter `r.net.prob`. School closures eliminate the school-based network (e.g. all transmission probabilities in school network to 0), increase familial contacts (e.g. within household), and reduce random edge generation from `r.net.prob` to `r.net.prob.sc`. Shelter in place reduces the generation of random edges to `r.net.prob.sip` and also increases household contacts **between household members who are not in essential workforce and therefore either work from home or are laid off (NOT IMPLEMENTED YET)**. The contact matrix is an $N\times N\times T$ array in which each $t$ slice contains a symmetric matrix with entries corresponding to the transmission probability between $i$ and $j$ were one of them to be infectious and the other susceptible.  

## Transmission  
The transmission model across the network is a modified SEIR that allows for variability in symptom severity by including presymptomatic ($I_P$), asymptomatic ($I_A$), mildly symptomatic ($I_M$), and severely symptomatic ($I_S$) states. This allows infection status to influence behaviors such as test-seeking and self-isolation and variability in transmissibility. Parameters fall under two categories, `t.___` parameters draw from a distribution to generate a time until the next event occurs and `p.___` parameters return a probability of transitioning to one state or another at each transition (e.g. asymptomatic or not). Probability of being asymptomatic and probability of severely symptomatic are age-dependent. As an example: `p.asymp` gets a probability that the infected individual will be asymptomatic, this probability is then used in a bernouli trial where success is defined as the individual transitioning to $I_A$ where they then draw from `t.asymp` to determine how long they will remain asymptomatic before recovering. 

Transition times are stored in the `t.til.nxt` matrix  and infection status is stored in the `inf.mat` matrix, each with dimensions $N\times T$. Each matrix is updated at the beginning of each model iteration. If an entry in the `t.til.nxt` matrix becomes negative, the function `next.state` is implemented to determine the next state and the time that will be spent in that state. Following these updates to infection status, the network is updated. School closures and shelter in place interventions alter the network as described above, and then random edges are built with function `add.r.edges` with probabilities influenced by interventions **and individual heterogeneities (NOT IMPLEMENTED YET)**. Finally, transmission is simulated across the network using function `new.infection` which reduces the contact matrix to only those columns corresonding to infectious individuals, then performs a Bernoulli trial row by row to simulate the probability a contact results in transmission. New infections are then added to `inf.mat` if the Bernoulli trial is successful and the corresponding row is susceptible. For these new infections, `t.latent` is also sampled and added to the `t.til.nxt` matrix.  

## Interventions  
Interventions such as school closures and shelter in place are currently implemented based on user input times (e.g. `t.sip` & `t.sip.end`) and affect the network as described above. Test-trace-isolate intervention occurs as follows: **NOT YET IMPLEMENTED, TO DISCUSS... user can input a matrix with columns corresponidng to time of testing, number of tests. How are tests allocated towards individuals at each time point?? Options include: random, based on their contact history, systematic (e.g. given N tests per month, N/4 individuals receive a test every fourth week)**

## Testing  

# Potential improvements/ideas (running list)    

## Network models to examine sub-epidemics (like Mission study), implications for effectiveness of SiP, opening back up  
Intro paper on network models [here](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5931789/pdf/nihms961617.pdf) with corresponding R package and [website with tutorials](http://www.epimodel.org/)  

#### Questions  
* What are the characteristics of a network that gives rise to the findings of the Mission study?  
* What are the implications of such a network for efficacy/limitations of SiP intervention? What are the implications for lifting SiP? i.e. how does the network change once SiP is lifted and how does transmission then propogate through this post-SiP network?  
* How can test-trace-isolate be used to limit transmission through such a network?  

#### Concepts  
* **Degree** or **connectivity** ($k$) - number of neighbors an individual has; **degree distribution** ($p_k$) population level characteristic of connectivity  
* **Size** of the network described by **distance** between two nodes - the length of the shortest path between them in the network; **diameter** of the network is the largest **distance** in the network  
* **Mixing matrix** describes how individuals of type $i$ within the network are connected to individuals of type $j$ where "type" can refer to any characteristic that differs between individuals. **Assortative mixing** describes a network pattern in which individuals are more likely to interact with like individuals  
* **local clustering** ($\phi$) - how many pairs are shared between pairs of individuals in the network, e.g. given two pairs, how many of the networks between the two pairs will look like triangles in which A relates to B, B relates to C, AND A also relates to C.  
* **Betweenness/Centrality** - Importance of particular individuals within the network; number of paths between $i$ and $j$ that pass through a particular node  

## Continuous time, stochastic model versions  
I've got a continuous time stochastic model built with the `adaptivetau` package that's got a slightly different structure that I could work on building out a bit more/make more user friendly with similar input/output functionality. Would having three models all running projections help or make things more confusing? Maybe have this going on behind the scenes and convey where there's more or less agreement between the models to DPHs?  

## Quantifying impact: how many cases/hospitalizations/deaths avoided due to SiP?  

## Erlang distributed latent period, hospitalization length  
Maybe a bit more of an academic exercise, but compartmental models make an implicit assumption that wait times between model states are exponentially distributed, which is rarely the case. Implementing "box cars", we can relax this assumption to make wait times Erlang (special case of a gamma distribution in which the shape parameter is a positive integer) distributed. Would be good for both $E$ and $H$ states given we have data on the distributions of latent period and hospitalization times we could fit to  

### Make some priors endogenous (e.g. non-adjustable), focus users on things most likely to vary by region/area/population:   

* Population Size  
* Start date of projection  
* End date of projection  
* Percent infected who are hospitalized  
* Average hospital length of stay  
* Timing and effectiveness ($\mathcal{R}_e$) of interventions  